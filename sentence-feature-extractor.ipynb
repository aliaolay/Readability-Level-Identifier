{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extractor (Sentence Level)\n",
    "_Click 'Run All' to extract text features from a sentence input._\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports & Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "#import libraries for traditional text features\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import re\n",
    "import syllables\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import stopwordsiso\n",
    "from stopwordsiso import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#import libraries for lexical text features\n",
    "import os\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "#SET UP FOR POS TAGGER\n",
    "# input local path to java.exe\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_341/bin/java.exe\" \n",
    "os.environ[\"JAVAHOME\"] = java_path\n",
    "\n",
    "#path to POS tagger jar\n",
    "jar_path = os.getcwd()\n",
    "jar =  jar_path + \"/stanford-postagger.jar\"\n",
    "\n",
    "# path to POS tagger model\n",
    "model_path = jar_path +\"/POSTagger/\"\n",
    "model = model_path + \"filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\"\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Input & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input:  hello hi, magandang umaga po, kumusta ka?\n",
      "Cleaned text:  hello hi magandang umaga po kumusta ka\n"
     ]
    }
   ],
   "source": [
    "#Get sentence input from user\n",
    "sentence_input = input(\"Input a sentence: \")\n",
    "print(\"Your input: \", sentence_input)\n",
    "\n",
    "#Remove punctuations\n",
    "clean_input = re.sub(r'[^\\w\\s]', '', sentence_input)\n",
    "print(\"Cleaned text: \", clean_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words:  7\n"
     ]
    }
   ],
   "source": [
    "words = sentence_input.split()\n",
    "print(\"Number of Words: \", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length:  5.0 letters\n"
     ]
    }
   ],
   "source": [
    "total_word_length = sum(len(word) for word in words)\n",
    "avg_word_length = total_word_length / len(words)\n",
    "\n",
    "print(\"Average Word Length: \", avg_word_length, \"letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total syllables in the text file: 14\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/itudidyay/Tagalog-Word-Syllabization-Python\n",
    "# https://pypi.org/project/syllables/\n",
    "\n",
    "vowels = 'aeiou'\n",
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "total_syllables = 0\n",
    "\n",
    "def count_syllables(text):\n",
    "    global total_syllables\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            if char in vowels:\n",
    "                total_syllables += 1\n",
    "        \n",
    "        # edge cases\n",
    "        if token == 'ng' or token == 'mga': # edge case ng, mga\n",
    "            total_syllables += 1\n",
    "        \n",
    "        elif (('io') in token): # edge case -io in names/surnames\n",
    "            total_syllables -= 1\n",
    "\n",
    "    return total_syllables\n",
    "\n",
    "def main():\n",
    "\n",
    "    total_syllables = count_syllables(clean_input)\n",
    "\n",
    "    print(f\"Total Syllables: {total_syllables}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Lexical Text Features\n",
    "#### Input Tokenization & POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text input\n",
    "words = nltk.word_tokenize(clean_input)\n",
    "\n",
    "#tag tokenized words\n",
    "tagged_words = pos_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOUN COUNT\n",
    "noun_count = 0\n",
    "for word, tag in tagged_words:\n",
    "    tag = tag.split('|')[-1] #removes word before |\n",
    "    if tag.startswith('NN'):\n",
    "        noun_count += 1\n",
    "    \n",
    "print(\"Number of nouns: \", noun_count)\n",
    "\n",
    "# NOUN TOKEN RATIO\n",
    "# = noun_count/total_token_count\n",
    "total_token_count = len(words)\n",
    "noun_token_ratio = noun_count/total_token_count\n",
    "\n",
    "print(\"Total number of tokens: \", total_token_count)\n",
    "print(\"Noun-Token Ratio: \", noun_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verb-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERB COUNT\n",
    "verb_count = 0\n",
    "for word, tag in tagged_words:\n",
    "    tag = tag.split('|')[-1] #removes word before |\n",
    "    if tag.startswith('VB'):\n",
    "        verb_count += 1\n",
    "        \n",
    "print(\"Number of verbs: \", verb_count)\n",
    "\n",
    "# VERB TOKEN RATIO\n",
    "# = verb_count/total_token_count\n",
    "total_token_count = len(words)\n",
    "verb_token_ratio = verb_count/total_token_count\n",
    "\n",
    "print(\"Total number of tokens: \", total_token_count)\n",
    "print(\"Noun-Token Ratio: \", verb_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique lexical categories\n",
    "unique_categories = set()\n",
    "for _, tag in tagged_words:\n",
    "    tag = tag.split('|')[-1] #removes word before |\n",
    "    if len(tag) >= 2:  # make sure the tag is not empty\n",
    "        category = tag[:2]  # extract the first two letters\n",
    "        unique_categories.add(category)\n",
    "\n",
    "print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "#NUMBER OF UNIQUE CATEGORIES\n",
    "num_categories = len(unique_categories)\n",
    "print(\"Number of Unique Categories:\", num_categories)\n",
    "\n",
    "# TOTAL NUM OF TOKENS\n",
    "total_token_count = len(words)\n",
    "\n",
    "# TYPE TOKEN RATIO\n",
    "ttr = num_categories/total_token_count\n",
    "print(\"Type-Token Ratio: \", ttr)\n",
    "\n",
    "#ROOT TTR\n",
    "root_ttr = num_categories/math.sqrt(total_token_count)\n",
    "print(\"Root Type-Token Ratio: \", root_ttr)\n",
    "\n",
    "#CORR TTR\n",
    "corr_ttr = num_categories/math.sqrt(2*total_token_count)\n",
    "print(\"Corrected Type-Token Ratio: \", corr_ttr)\n",
    "\n",
    "#BILOGARITHMIC TTR\n",
    "log_ttr = math.log(num_categories)/math.log(total_token_count)\n",
    "print(\"Bilogarithmic Type-Token Ratio: \", log_ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF LEXICAL WORDS\n",
    "# count number of nouns, verbs, adjectives, and adverbs\n",
    "num_lexwords = 0\n",
    "for word, tag in tagged_words:\n",
    "    tag = tag.split('|')[-1] #removes word before |\n",
    "    if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "        num_lexwords += 1\n",
    "        \n",
    "print(\"Number of lexical words: \", num_lexwords)\n",
    "\n",
    "# LEXICAL DENSITY\n",
    "# = lex_density/total_token_count\n",
    "total_token_count = len(words)\n",
    "lex_density = num_lexwords/total_token_count\n",
    "\n",
    "print(\"Total number of tokens: \", total_token_count)\n",
    "print(\"Lexical Density: \", lex_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foreign Word-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOREIGN WORD COUNT\n",
    "fw_count = 0\n",
    "for word, tag in tagged_words:\n",
    "    tag = tag.split('|')[-1] #removes word before |\n",
    "    if tag.startswith('FW'):\n",
    "        fw_count += 1\n",
    "        \n",
    "print(\"Number of foreign words: \", fw_count)\n",
    "\n",
    "# FOREIGN WORD - TOKEN RATIO\n",
    "# = fw_count/total_token_count\n",
    "total_token_count = len(words)\n",
    "fw_token_ratio = fw_count/total_token_count\n",
    "\n",
    "print(\"Total number of tokens: \", total_token_count)\n",
    "print(\"Foreign Word-Token Ratio: \", fw_token_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
