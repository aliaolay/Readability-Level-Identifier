{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import stopwordsiso\n",
    "from stopwordsiso import stopwords\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"clean-txt\"\n",
    "\n",
    "# all words from files\n",
    "sentences = []\n",
    "\n",
    "# populates sentences with all texts\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            words = word_tokenize(text)\n",
    "            sentences.extend(words)\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords('tl'))\n",
    "\n",
    "# preprocessing\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    processed_sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the word2vec model\n",
    "model = Word2Vec(processed_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# save\n",
    "model.save(\"filipino_word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "model = Word2Vec.load(\"filipino_word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('itinanghal', 0.34606117010116577), ('hinarang', 0.34141793847084045), ('tinitirahang', 0.3247967064380646), ('hanapin', 0.32373079657554626), ('pagsusulit', 0.3082212507724762), ('igagalang', 0.3019391894340515), ('droga', 0.29676660895347595), ('malipol', 0.2953234612941742), ('malalayo', 0.2875189483165741), ('binondo', 0.28147175908088684)]\n",
      "-0.06799804\n"
     ]
    }
   ],
   "source": [
    "# uses\n",
    "\n",
    "# get similar words\n",
    "similar_words = model.wv.most_similar(\"hapon\", topn=10)\n",
    "print(similar_words)\n",
    "\n",
    "# get similarity between two words\n",
    "cosine_similarity = model.wv.similarity('babae', 'lalaki')\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /Users/jerseydayao/opt/anaconda3/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /Users/jerseydayao/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/jerseydayao/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/jerseydayao/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (63.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# pre-trained model\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "model = fasttext.load_model('cc.tl.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubo-kubo\n",
      "inuupahang\n",
      "titirahan\n",
      "resthouse\n",
      "gatehouse\n",
      "boardinghouse\n",
      "Mansyon\n",
      "kubo\n",
      "apartment\n",
      "pinauupahan\n",
      "inuupahan\n",
      "Treehouse\n",
      "tinutuluyan\n",
      "townhouse\n",
      "treehouse\n",
      "unuupahan\n",
      "bunkhouse\n",
      "kubong\n",
      "kwarto\n",
      "makisilong\n",
      "garahe\n",
      "tahanan\n",
      "Paguwi\n",
      "staffhouse\n",
      "pinapaupahan\n",
      "safehouse\n",
      "[(0.6491464972496033, 'kubo-kubo'), (0.6266229748725891, 'inuupahang'), (0.6241036653518677, 'titirahan'), (0.6082715392112732, 'resthouse'), (0.6074135899543762, 'gatehouse'), (0.607253909111023, 'boardinghouse'), (0.5948531627655029, 'Mansyon'), (0.5940391421318054, 'kubo'), (0.5937168598175049, 'apartment'), (0.5917840003967285, 'pinauupahan'), (0.59103924036026, 'inuupahan'), (0.5894445180892944, 'Treehouse'), (0.5871586799621582, 'tinutuluyan'), (0.5829326510429382, 'townhouse'), (0.5817272663116455, 'treehouse'), (0.5778210163116455, 'unuupahan'), (0.5770028829574585, 'bunkhouse'), (0.5768028497695923, 'kubong'), (0.5718590617179871, 'kwarto'), (0.5712212324142456, 'makisilong'), (0.5663864612579346, 'garahe'), (0.5652022957801819, 'tahanan'), (0.5640914440155029, 'Paguwi'), (0.5584996342658997, 'staffhouse'), (0.5583826899528503, 'pinapaupahan'), (0.5577643513679504, 'safehouse')]\n",
      "[(0.6235178709030151, 'babae')]\n"
     ]
    }
   ],
   "source": [
    "# uses\n",
    "\n",
    "find = \"bahay\"\n",
    "similar = model.get_nearest_neighbors(find, k=100)                      # this is more like a bypass to not get repeated words\n",
    "filtered = [word for word in similar if find not in word[1]]    # filter\n",
    "filitered = filtered[:20]                                # gets top 20 based on the filter\n",
    "\n",
    "# may want to implement for django\n",
    "# list of words, instead of embedding + word\n",
    "for item in filtered:\n",
    "    print(item[1])\n",
    "\n",
    "# prints out array of embedding_value-word pair\n",
    "print(filtered)\n",
    "\n",
    "# example\n",
    "# reyna : hari = babae : lalaki (etc.)\n",
    "analogies = model.get_analogies('reyna', 'hari', 'lalaki', k=1) # default k == 1\n",
    "print(analogies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
