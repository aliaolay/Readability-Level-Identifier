{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extractor (Book Level)\n",
    "_Click 'Run All' to extract text features from a book input._\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for traditional text features\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import re\n",
    "import syllables\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import stopwordsiso\n",
    "from stopwordsiso import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#import libraries for lexical text features\n",
    "import os\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "\n",
    "\n",
    "# should work now without having to change the path every time a new user runs the program\n",
    "curr_path = os.getcwd().replace('\\clean-txt', '')\n",
    "\n",
    "# POS-Tagger SET UP\n",
    "# input local path to java.exe\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_341/bin/java.exe\" \n",
    "os.environ[\"JAVAHOME\"] = java_path\n",
    "\n",
    "#path to POS tagger jar\n",
    "\n",
    "os.chdir(curr_path)\n",
    "\n",
    "print(curr_path)\n",
    "jar =  curr_path + \"/stanford-postagger.jar\"\n",
    "\n",
    "# path to POS tagger model\n",
    "model_path = curr_path +\"/POSTagger/\"\n",
    "model = model_path + \"filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\"\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.getcwd() + \"/clean-txt/\"      # gets path to 'clean-txt' directory\n",
    "# filename = input(\"Input text filename: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = nltk.data.find(path)\n",
    "# corpusReader = nltk.corpus.PlaintextCorpusReader(folder, filename)\n",
    "\n",
    "# print(\"Number of Sentences: \", len(corpusReader.sents()))\n",
    "\n",
    "def sentence_count(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    return len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path + \"/\" + filename, 'r') as file:\n",
    "#     word_length = [len(word) for line in file for word in line.rstrip().split(\" \")]\n",
    "#     word_avg = sum(word_length)/len(word_length)\n",
    "    \n",
    "# print(\"Average Word Length: \", word_avg, \"letters\")\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    avg = total_word_length / len(words)\n",
    "\n",
    "    return avg\n",
    "\n",
    "# print(\"Average Word Length: \", avg_word_length, \"letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folder = nltk.data.find(path)\n",
    "# corpusReader = nltk.corpus.PlaintextCorpusReader(folder, filename)\n",
    "\n",
    "# # SOURCE: https://stackoverflow.com/questions/35900029/average-sentence-length-for-every-text-in-corpus-python3-nltk\n",
    "# avg = sum(len(sent) for sent in corpusReader.sents()) / len(corpusReader.sents())\n",
    "# print(\"Average Sentence Length: \", avg, \"words\")\n",
    "\n",
    "def avg_sent_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    avg = sum(len(sent.split()) for sent in sentences) / len(sentences)\n",
    "\n",
    "    return avg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/itudidyay/Tagalog-Word-Syllabization-Python\n",
    "# https://pypi.org/project/syllables/\n",
    "\n",
    "vowels = 'aeiou'\n",
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def count_syllables(text):\n",
    "\n",
    "    total_syllables = 0\n",
    "    monosyl_count = 0\n",
    "    polysyl_count = 0\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        syllable_count = 0\n",
    "        for char in token:\n",
    "            if char.lower() in vowels:\n",
    "                total_syllables += 1\n",
    "                syllable_count += 1\n",
    "        \n",
    "        # edge cases\n",
    "        if token == 'ng' or token == 'mga': # edge case ng, mga\n",
    "            total_syllables += 1\n",
    "            syllable_count += 1\n",
    "        \n",
    "        elif (('io') in token): # edge case -io in names/surnames\n",
    "            total_syllables -= 1\n",
    "            syllable_count -= 1\n",
    "            \n",
    "        if syllable_count == 1:\n",
    "            monosyl_count += 1\n",
    "        elif syllable_count > 1:\n",
    "            polysyl_count += 1\n",
    "\n",
    "    return total_syllables, monosyl_count, polysyl_count\n",
    "\n",
    "# def main():\n",
    "#     total_syllables = count_syllables(clean_input)\n",
    "\n",
    "#     print(f\"Total syllables in the text file: {total_syllables}\")\n",
    "#     print(f\"Number of monosyllabic words: {monosyl_count}\")\n",
    "#     print(f\"Number of polysyllabic words: {polysyl_count}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency\n",
    "> _Outputs will be placed in the 'word-freq output' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Read the text file\n",
    "# with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "def word_freq(path, filename):\n",
    "    stop_words = set(stopwords('tl'))\n",
    "    \n",
    "    with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        text = file.read()\n",
    "        text_tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word.lower() for word in text_tokens if word.lower() not in stop_words] #removes stopwords\n",
    "        text_tokens = [word for word in filtered_tokens if word.isalnum()] # removes punctuation marks\n",
    "        fdist = FreqDist(text_tokens)\n",
    "\n",
    "        # Create a DataFrame from the frequency distribution\n",
    "        df_fdist = pd.DataFrame.from_dict(fdist, orient='index', columns=['Frequency'])\n",
    "        df_fdist.index.name = 'Word'\n",
    "\n",
    "        # Sort the DataFrame by frequency in descending order\n",
    "        df_fdist_sorted = df_fdist.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "        #print(df_fdist_sorted)\n",
    "\n",
    "\n",
    "        out_path = path.replace('/clean-txt', '/word-freq output')\n",
    "        out_filename = \"[wordfreq] \" + filename.removesuffix('_cleaned.txt') + \".csv\"\n",
    "        df_fdist_sorted.to_csv(os.path.join(out_path, out_filename), encoding='utf-8')\n",
    "\n",
    "        #load in the dataframe\n",
    "        df = pd.read_csv(os.path.join(out_path, out_filename), index_col=0)\n",
    "        df.head(500)\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10)\n",
    "        wordcloud.generate(' '.join(text_tokens))\n",
    "\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        wordcloud.to_file(out_path + \"/wordcloud/\" + filename.removesuffix('_cleaned.txt') + \".png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Lexical Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries & POS Tagger Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #SET UP FOR POS TAGGER\n",
    "# # input local path to java.exe\n",
    "# java_path = \"C:/Program Files/Java/jre1.8.0_341/bin/java.exe\" \n",
    "# os.environ[\"JAVAHOME\"] = java_path\n",
    "\n",
    "# #path to POS tagger jar\n",
    "# jar_path = os.getcwd()\n",
    "# jar =  jar_path + \"/stanford-postagger.jar\"\n",
    "\n",
    "# # path to POS tagger model\n",
    "# model_path = jar_path +\"/POSTagger/\"\n",
    "# model = model_path + \"filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\"\n",
    "\n",
    "# pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Tokenization & POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# words = nltk.word_tokenize(text)\n",
    "\n",
    "# #tag tokenized words\n",
    "# tagged_words = pos_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOUN COUNT\n",
    "# noun_count = 0\n",
    "# for word, tag in tagged_words:\n",
    "#     tag = tag.split('|')[-1] #removes word before |\n",
    "#     if tag.startswith('NN'):\n",
    "#         noun_count += 1\n",
    "    \n",
    "# print(\"Number of nouns: \", noun_count)\n",
    "\n",
    "# # NOUN TOKEN RATIO\n",
    "# # = noun_count/total_token_count\n",
    "# total_token_count = len(words)\n",
    "# noun_token_ratio = noun_count/total_token_count\n",
    "\n",
    "# print(\"Total number of tokens: \", total_token_count)\n",
    "# print(\"Noun-Token Ratio: \", noun_token_ratio)\n",
    "\n",
    "def ntr(words, tagged):\n",
    "\n",
    "    # NOUN COUNT\n",
    "    noun_count = 0\n",
    "    for word, tag in tagged:\n",
    "        tag = tag.split('|')[-1] #removes word before |\n",
    "        if tag.startswith('NN'):\n",
    "            noun_count += 1\n",
    "        \n",
    "    # print(\"Number of nouns: \", noun_count)\n",
    "\n",
    "    # NOUN TOKEN RATIO\n",
    "    # = noun_count/total_token_count\n",
    "    total_token_count = len(words)\n",
    "    noun_token_ratio = noun_count/total_token_count\n",
    "\n",
    "    return noun_token_ratio\n",
    "\n",
    "# print(\"Total number of tokens: \", total_token_count)\n",
    "# print(\"Noun-Token Ratio: \", noun_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verb-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtr(words, tagged):\n",
    "    # VERB COUNT\n",
    "    verb_count = 0\n",
    "    for word, tag in tagged:\n",
    "        tag = tag.split('|')[-1] #removes word before |\n",
    "        if tag.startswith('VB'):\n",
    "            verb_count += 1\n",
    "            \n",
    "    # print(\"Number of verbs: \", verb_count)\n",
    "\n",
    "    # VERB TOKEN RATIO\n",
    "    # = verb_count/total_token_count\n",
    "    total_token_count = len(words)\n",
    "    verb_token_ratio = verb_count/total_token_count\n",
    "\n",
    "    return verb_token_ratio\n",
    "\n",
    "# print(\"Total number of tokens: \", total_token_count)\n",
    "# print(\"Noun-Token Ratio: \", verb_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(words, tagged):\n",
    "# count unique lexical categories\n",
    "    unique_categories = set()\n",
    "    for _, tag in tagged:\n",
    "        tag = tag.split('|')[-1] #removes word before |\n",
    "        if len(tag) >= 2:  # make sure the tag is not empty\n",
    "            category = tag[:2]  # extract the first two letters\n",
    "            unique_categories.add(category)\n",
    "\n",
    "    # print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "    #NUMBER OF UNIQUE CATEGORIES\n",
    "    num_categories = len(unique_categories)\n",
    "    # print(\"Number of Unique Categories:\", num_categories)\n",
    "\n",
    "    # TOTAL NUM OF TOKENS\n",
    "    total_token_count = len(words)\n",
    "\n",
    "    # TYPE TOKEN RATIO\n",
    "    ttr = num_categories/total_token_count\n",
    "    # print(\"Type-Token Ratio: \", ttr)\n",
    "\n",
    "    #ROOT TTR\n",
    "    root_ttr = num_categories/math.sqrt(total_token_count)\n",
    "    # print(\"Root Type-Token Ratio: \", root_ttr)\n",
    "\n",
    "    #CORR TTR\n",
    "    corr_ttr = num_categories/math.sqrt(2*total_token_count)\n",
    "    # print(\"Corrected Type-Token Ratio: \", corr_ttr)\n",
    "\n",
    "    #BILOGARITHMIC TTR\n",
    "    denominator = math.log(total_token_count)\n",
    "\n",
    "    if denominator == 0:\n",
    "        log_ttr = 0\n",
    "    else:\n",
    "        log_ttr = math.log(num_categories)/math.log(total_token_count)\n",
    "    # print(\"Bilogarithmic Type-Token Ratio: \", log_ttr)\n",
    "\n",
    "    return ttr, root_ttr, corr_ttr, log_ttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lexical_density(words, tagged):\n",
    "\n",
    "    # NUMBER OF LEXICAL WORDS\n",
    "    # count number of nouns, verbs, adjectives, and adverbs\n",
    "    num_lexwords = 0\n",
    "    for word, tag in tagged:\n",
    "        tag = tag.split('|')[-1] #removes word before |\n",
    "        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            num_lexwords += 1\n",
    "            \n",
    "    # print(\"Number of lexical words: \", num_lexwords)\n",
    "\n",
    "    # LEXICAL DENSITY\n",
    "    # = lex_density/total_token_count\n",
    "    total_token_count = len(words)\n",
    "    lex_density = num_lexwords/total_token_count\n",
    "\n",
    "    return lex_density\n",
    "\n",
    "# print(\"Total number of tokens: \", total_token_count)\n",
    "# print(\"Lexical Density: \", lex_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foreign Word-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwtr(words, tagged):\n",
    "    # FOREIGN WORD COUNT\n",
    "    fw_count = 0\n",
    "    for word, tag in tagged:\n",
    "        tag = tag.split('|')[-1] #removes word before |\n",
    "        if tag.startswith('FW'):\n",
    "            fw_count += 1\n",
    "            \n",
    "    # print(\"Number of foreign words: \", fw_count)\n",
    "\n",
    "    # FOREIGN WORD - TOKEN RATIO\n",
    "    # = fw_count/total_token_count\n",
    "    total_token_count = len(words)\n",
    "    fw_token_ratio = fw_count/total_token_count\n",
    "\n",
    "    return fw_token_ratio\n",
    "\n",
    "# print(\"Total number of tokens: \", total_token_count)\n",
    "# print(\"Foreign Word-Token Ratio: \", fw_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book-Level Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = curr_path + '/clean-txt'\n",
    "os.chdir(path)\n",
    "\n",
    "csv_header = ['Book Title', 'Word Count', 'Sentence Count', 'AVG Word Length', 'AVG Sentence Length', 'Total Syllables', 'MONOSYLL', 'POLYSYLL', 'NTR', 'VTR', 'TTR', 'Root TTR', 'Corrected TTR', 'BiLog TTR', 'LD', 'FWTR', 'MIN', 'MAX']\n",
    "data = []\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \n",
    "    path, file_name = os.path.split(file_path)\n",
    "    suffix = '_cleaned.txt'\n",
    "    if file_name.lower().endswith(suffix.lower()):\n",
    "        file_name = file_name[: -len(suffix)]   # e.g. Tahan na Tahanan\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        \n",
    "        \n",
    "        print('-------------------------------------------------')\n",
    "        print(file_name)\n",
    "        print('-------------------------------------------------')\n",
    "        \n",
    "        sentence = 0\n",
    "        min = 0\n",
    "        max = 0\n",
    "        for line in file:\n",
    "\n",
    "            \n",
    "            # added the age at Line 0 of each sentence token script          \n",
    "            if sentence == 0:\n",
    "                age = line.strip()  # output: X-Y\n",
    "                \n",
    "                min_str, max_str = age.split(\"-\")\n",
    "\n",
    "                min = int(min_str)\n",
    "                max = int(max_str)\n",
    "\n",
    "                sentence += 1\n",
    "                continue\n",
    "\n",
    "            csv_data = [file_name]\n",
    "\n",
    "            # print('Sentence ', sentence)\n",
    "\n",
    "            # TRAD\n",
    "            # print('WORD COUNT: ', word_count(line))\n",
    "            csv_data.append(word_count(line))\n",
    "            \n",
    "            # print('SENTENCE COUNT: ', sentence_count(line))\n",
    "            csv_data.append(sentence_count(line))\n",
    "\n",
    "             # print('AVG WORD LENGTH: ', avg_word_length(line))\n",
    "            csv_data.append(avg_word_length(line))\n",
    "            \n",
    "            # print('AVG SENTENCE LENGTH: ', avg_sent_length(line))\n",
    "            csv_data.append(avg_sent_length(line))\n",
    "\n",
    "            # print('TOTAL SYLLABLES: ', count_syllables(line)[0])\n",
    "            # print('MONOSYLLABIC: ', count_syllables(line)[1])\n",
    "            # print('POLYSYLLABIC: ', count_syllables(line)[2])\n",
    "            csv_data.append(count_syllables(line)[0])\n",
    "            csv_data.append(count_syllables(line)[1])\n",
    "            csv_data.append(count_syllables(line)[2])\n",
    "\n",
    "\n",
    "            # LEX\n",
    "            wordsss = nltk.word_tokenize(line)\n",
    "            tagged_words = pos_tagger.tag(wordsss)\n",
    "            # print('NTR: ', ntr(wordsss, tagged_words))\n",
    "            csv_data.append(ntr(wordsss, tagged_words))\n",
    "\n",
    "            # print('VTR: ', vtr(wordsss, tagged_words))\n",
    "            csv_data.append(vtr(wordsss, tagged_words))\n",
    "\n",
    "            # print('TTR: ', ttr(wordsss, tagged_words)[0])\n",
    "            # print('Root-TTR: ', ttr(wordsss, tagged_words)[1])\n",
    "            # print('Corrected-TTR: ', ttr(wordsss, tagged_words)[2])\n",
    "            # print('Bilogarithmic-TTR: ', ttr(wordsss, tagged_words)[3])\n",
    "            csv_data.append(ttr(wordsss, tagged_words)[0])\n",
    "            csv_data.append(ttr(wordsss, tagged_words)[1])\n",
    "            csv_data.append(ttr(wordsss, tagged_words)[2])\n",
    "            csv_data.append(ttr(wordsss, tagged_words)[3])\n",
    "\n",
    "            # print('Lexical Density: ', lexical_density(wordsss, tagged_words))\n",
    "            csv_data.append(lexical_density(wordsss, tagged_words))\n",
    "\n",
    "            # print('FWTR: ', fwtr(wordsss, tagged_words))\n",
    "            csv_data.append(fwtr(wordsss, tagged_words))\n",
    "\n",
    "            csv_data.append(min)\n",
    "            csv_data.append(max)\n",
    "            data.append(csv_data)\n",
    "            print(data)\n",
    "\n",
    "    return '0'\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = f'{path}/{file}'\n",
    "        read_text_file(file_path)\n",
    "\n",
    "main = curr_path\n",
    "os.chdir(main)\n",
    "\n",
    "df = pd.DataFrame(data, columns = csv_header)\n",
    "df.to_csv('book.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For wordcloud\n",
    "curr_path = os.getcwd().replace('\\clean-txt', '')\n",
    "\n",
    "clean_txt_path = curr_path + '/clean-txt'\n",
    "os.chdir(curr_path)\n",
    "print(curr_path)\n",
    "\n",
    "for file in os.listdir(clean_txt_path):\n",
    "    if file.endswith('.txt'):\n",
    "        word_freq(clean_txt_path, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
