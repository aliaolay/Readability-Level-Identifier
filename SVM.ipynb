{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Sentence Count</th>\n",
       "      <th>Ave. Word Length</th>\n",
       "      <th>Ave. Sentence Length</th>\n",
       "      <th>Total Syllables</th>\n",
       "      <th>Noun-Token Ratio</th>\n",
       "      <th>Verb-Token Ratio</th>\n",
       "      <th>Type-Token Ratio</th>\n",
       "      <th>Root TTR</th>\n",
       "      <th>Corrected TTR</th>\n",
       "      <th>Bilogarithmic TTR</th>\n",
       "      <th>Lexical Density</th>\n",
       "      <th>FW-Token Ratio</th>\n",
       "      <th>Age Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>973</td>\n",
       "      <td>85</td>\n",
       "      <td>5.216380</td>\n",
       "      <td>13.823529</td>\n",
       "      <td>2096</td>\n",
       "      <td>0.212914</td>\n",
       "      <td>0.113438</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.324938</td>\n",
       "      <td>0.229766</td>\n",
       "      <td>0.340415</td>\n",
       "      <td>0.403141</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>937</td>\n",
       "      <td>95</td>\n",
       "      <td>5.048168</td>\n",
       "      <td>12.105263</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.171946</td>\n",
       "      <td>0.103167</td>\n",
       "      <td>0.009955</td>\n",
       "      <td>0.330911</td>\n",
       "      <td>0.233990</td>\n",
       "      <td>0.342185</td>\n",
       "      <td>0.412670</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900</td>\n",
       "      <td>72</td>\n",
       "      <td>4.793478</td>\n",
       "      <td>15.097222</td>\n",
       "      <td>1844</td>\n",
       "      <td>0.136999</td>\n",
       "      <td>0.105312</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.335809</td>\n",
       "      <td>0.237453</td>\n",
       "      <td>0.343626</td>\n",
       "      <td>0.367195</td>\n",
       "      <td>0.033551</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1279</td>\n",
       "      <td>97</td>\n",
       "      <td>4.959199</td>\n",
       "      <td>16.319588</td>\n",
       "      <td>2596</td>\n",
       "      <td>0.192617</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>0.284970</td>\n",
       "      <td>0.201504</td>\n",
       "      <td>0.328185</td>\n",
       "      <td>0.405369</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>876</td>\n",
       "      <td>75</td>\n",
       "      <td>4.849498</td>\n",
       "      <td>14.053333</td>\n",
       "      <td>1824</td>\n",
       "      <td>0.195192</td>\n",
       "      <td>0.103846</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.341096</td>\n",
       "      <td>0.241191</td>\n",
       "      <td>0.345171</td>\n",
       "      <td>0.388462</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1177</td>\n",
       "      <td>137</td>\n",
       "      <td>5.007531</td>\n",
       "      <td>10.883212</td>\n",
       "      <td>2491</td>\n",
       "      <td>0.178177</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.289074</td>\n",
       "      <td>0.204406</td>\n",
       "      <td>0.329475</td>\n",
       "      <td>0.381906</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1601</td>\n",
       "      <td>120</td>\n",
       "      <td>5.328449</td>\n",
       "      <td>15.816667</td>\n",
       "      <td>3610</td>\n",
       "      <td>0.175055</td>\n",
       "      <td>0.143326</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>0.257279</td>\n",
       "      <td>0.181924</td>\n",
       "      <td>0.319252</td>\n",
       "      <td>0.407549</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1365</td>\n",
       "      <td>132</td>\n",
       "      <td>4.945166</td>\n",
       "      <td>12.393939</td>\n",
       "      <td>2896</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.134395</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.277615</td>\n",
       "      <td>0.196303</td>\n",
       "      <td>0.325853</td>\n",
       "      <td>0.407643</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>975</td>\n",
       "      <td>92</td>\n",
       "      <td>4.995984</td>\n",
       "      <td>12.347826</td>\n",
       "      <td>2089</td>\n",
       "      <td>0.145889</td>\n",
       "      <td>0.139699</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>0.231284</td>\n",
       "      <td>0.341053</td>\n",
       "      <td>0.416446</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>552</td>\n",
       "      <td>69</td>\n",
       "      <td>4.984211</td>\n",
       "      <td>10.231884</td>\n",
       "      <td>1177</td>\n",
       "      <td>0.129851</td>\n",
       "      <td>0.129851</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.424967</td>\n",
       "      <td>0.300497</td>\n",
       "      <td>0.368494</td>\n",
       "      <td>0.374627</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word Count  Sentence Count  Ave. Word Length  Ave. Sentence Length   \n",
       "0         973              85          5.216380             13.823529  \\\n",
       "1         937              95          5.048168             12.105263   \n",
       "2         900              72          4.793478             15.097222   \n",
       "3        1279              97          4.959199             16.319588   \n",
       "4         876              75          4.849498             14.053333   \n",
       "5        1177             137          5.007531             10.883212   \n",
       "6        1601             120          5.328449             15.816667   \n",
       "7        1365             132          4.945166             12.393939   \n",
       "8         975              92          4.995984             12.347826   \n",
       "9         552              69          4.984211             10.231884   \n",
       "\n",
       "   Total Syllables  Noun-Token Ratio  Verb-Token Ratio  Type-Token Ratio   \n",
       "0             2096          0.212914          0.113438          0.009599  \\\n",
       "1             2018          0.171946          0.103167          0.009955   \n",
       "2             1844          0.136999          0.105312          0.010252   \n",
       "3             2596          0.192617          0.100000          0.007383   \n",
       "4             1824          0.195192          0.103846          0.010577   \n",
       "5             2491          0.178177          0.104972          0.007597   \n",
       "6             3610          0.175055          0.143326          0.006018   \n",
       "7             2896          0.191083          0.134395          0.007006   \n",
       "8             2089          0.145889          0.139699          0.009726   \n",
       "9             1177          0.129851          0.129851          0.016418   \n",
       "\n",
       "   Root TTR  Corrected TTR  Bilogarithmic TTR  Lexical Density   \n",
       "0  0.324938       0.229766           0.340415         0.403141  \\\n",
       "1  0.330911       0.233990           0.342185         0.412670   \n",
       "2  0.335809       0.237453           0.343626         0.367195   \n",
       "3  0.284970       0.201504           0.328185         0.405369   \n",
       "4  0.341096       0.241191           0.345171         0.388462   \n",
       "5  0.289074       0.204406           0.329475         0.381906   \n",
       "6  0.257279       0.181924           0.319252         0.407549   \n",
       "7  0.277615       0.196303           0.325853         0.407643   \n",
       "8  0.327086       0.231284           0.341053         0.416446   \n",
       "9  0.424967       0.300497           0.368494         0.374627   \n",
       "\n",
       "   FW-Token Ratio  Age Classification  \n",
       "0        0.017452                  10  \n",
       "1        0.021719                  10  \n",
       "2        0.033551                   9  \n",
       "3        0.015436                  12  \n",
       "4        0.001923                   9  \n",
       "5        0.016575                  12  \n",
       "6        0.004376                   9  \n",
       "7        0.012102                  12  \n",
       "8        0.002653                  10  \n",
       "9        0.007463                   9  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOURCE: https://stackoverflow.com/questions/54820210/how-to-train-svm-model-in-sklearn-python-by-input-csv-file\n",
    "#SOURCE: https://databasetown.com/implementing-support-vector-machine-svm-in-python/\n",
    "path = path = os.getcwd()\n",
    "text_features = pandas.read_csv(path + \"/extracted_TRAD_LEX.csv\")\n",
    "text_features_header = [\"Word Count\", \"Sentence Count\", \"Ave. Word Length\", \"Ave. Sentence Length\", \"Total Syllables\", \"Noun-Token Ratio\",\n",
    "                        \"Verb-Token Ratio\", \"Type-Token Ratio\", \"Root TTR\", \"Corrected TTR\", \"Bilogarithmic TTR\", \"Lexical Density\",\n",
    "                        \"FW-Token Ratio\", \"Age Classification\"]\n",
    "\n",
    "text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.17700000e+03 1.37000000e+02 5.00753138e+00 1.08832117e+01\n",
      "  2.49100000e+03 1.78176796e-01 1.04972376e-01 7.59668508e-03\n",
      "  2.89073582e-01 2.04405890e-01 3.29474514e-01 3.81906077e-01\n",
      "  1.65745856e-02]\n",
      " [9.73000000e+02 8.50000000e+01 5.21638018e+00 1.38235294e+01\n",
      "  2.09600000e+03 2.12914485e-01 1.13438045e-01 9.59860384e-03\n",
      "  3.24937905e-01 2.29765796e-01 3.40415116e-01 4.03141361e-01\n",
      "  1.74520070e-02]\n",
      " [1.36500000e+03 1.32000000e+02 4.94516595e+00 1.23939394e+01\n",
      "  2.89600000e+03 1.91082803e-01 1.34394904e-01 7.00636943e-03\n",
      "  2.77614956e-01 1.96303418e-01 3.25852748e-01 4.07643312e-01\n",
      "  1.21019108e-02]\n",
      " [9.00000000e+02 7.20000000e+01 4.79347826e+00 1.50972222e+01\n",
      "  1.84400000e+03 1.36999068e-01 1.05312209e-01 1.02516309e-02\n",
      "  3.35809381e-01 2.37453090e-01 3.43625942e-01 3.67194781e-01\n",
      "  3.35507922e-02]\n",
      " [5.52000000e+02 6.90000000e+01 4.98421053e+00 1.02318841e+01\n",
      "  1.17700000e+03 1.29850746e-01 1.29850746e-01 1.64179104e-02\n",
      "  4.24967075e-01 3.00497101e-01 3.68494381e-01 3.74626866e-01\n",
      "  7.46268657e-03]\n",
      " [8.76000000e+02 7.50000000e+01 4.84949833e+00 1.40533333e+01\n",
      "  1.82400000e+03 1.95192308e-01 1.03846154e-01 1.05769231e-02\n",
      "  3.41095520e-01 2.41190955e-01 3.45171090e-01 3.88461538e-01\n",
      "  1.92307692e-03]\n",
      " [1.27900000e+03 9.70000000e+01 4.95919938e+00 1.63195876e+01\n",
      "  2.59600000e+03 1.92617450e-01 1.00000000e-01 7.38255034e-03\n",
      "  2.84970268e-01 2.01504409e-01 3.28185173e-01 4.05369127e-01\n",
      "  1.54362416e-02]\n",
      " [1.60100000e+03 1.20000000e+02 5.32844933e+00 1.58166667e+01\n",
      "  3.61000000e+03 1.75054705e-01 1.43326039e-01 6.01750547e-03\n",
      "  2.57279148e-01 1.81923830e-01 3.19252080e-01 4.07549234e-01\n",
      "  4.37636761e-03]]\n"
     ]
    }
   ],
   "source": [
    "X = text_features[text_features_header[:-1]].values\n",
    "Y = text_features[['Age Classification']].values.ravel()\n",
    "\n",
    "x_train , x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(x_train, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Polynomial Kernel):  0.00\n",
      "F1 (Polynomial Kernel):  0.00\n",
      "Accuracy (RBF Kernel):  0.00\n",
      "F1 (RBF Kernel):  0.00\n"
     ]
    }
   ],
   "source": [
    "poly_pred = poly.predict(x_test)\n",
    "rbf_pred = rbf.predict(x_test)\n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
